# This file should contain everything to configure the workflow on a global scale.
# In case of sample based data, it should be complemented by a samples.tsv file that contains
# one row per sample. It can be parsed easily via pandas.
wdir: "/vol/cloud/agnostos-wf/db_update"
rdir: "/vol/cloud/db_update"
data: "/vol/cloud/data"

# Cluster update configuration
# Setting update to TRUE to run the cluster-update version of the snakemake workflow (otherwise FALSE or empty)
update: "TRUE"
# Path to original cluster DB (sequence and cluster DBs)
ordir: "/vol/cloud/agnostosDB"
# Path to the sequences that need to be integrated
# Formats: name_contigs.fasta or name_genes.fasta
new_data: "/vol/cloud/data/Collection_giant_viruses_genes.fasta"
# specify at which stage are your data, can be either "genes" or "contigs"
new_data_stage: "genes" #"genes" or "contigs"
# If you alrady have the gene predictions, please provide path to gene completeness information
new_data_partial: "/vol/cloud/data/Collection_giant_viruses_genes_partial_info.tsv"

# Threads configuration
threads_default: 28
threads_collect: 28
threads_cat_ref: 4
threads_res: 14
# Databases
pfam_db: "/vol/cloud/databases/Pfam-A.hmm"
pfam_clan: "/vol/cloud/databases/Pfam-A.clans.tsv.gz"
antifam_db: "/vol/cloud/databases/AntiFam.hmm"
uniref90_db: "/vol/cloud/databases/uniref90.fasta.gz"
nr_db: "/vol/cloud/databases/nr.fasta.gz"
uniclust_db: "/vol/scratch/DBs/uniclust30_2017_10/uniclust30_2017_10"
uniprot_db: "/vol/cloud/databases/uniprotKB.fasta.gz"
pfam_hh_db: "/vol/cloud/databases/pfam"
DPD: "/vol/cloud/databases/dpd_uniprot_sprot.fasta"
db_dir: "/vol/cloud/databases/"
taxdb: "UniProtKB"

# Files retrieved from the databases
# List of shared reduced Pfam domain names (dowloadable from Figshare..)
pfam_shared_terms: "/vol/cloud/databases/pfam_files/Pfam-31_names_mod_01122019.tsv"
# Created using the protein accessions and the descriptions found on the fasta headers
uniref90_prot: "/vol/cloud/databases/uniref90.proteins.tsv.gz"
nr_prot: "/vol/cloud/databases/nr.proteins.tsv.gz"
# Information dowloaded from Dataset-S1 from the DPD paper:
dpd_info: "/vol/cloud/databases/dpd_ids_all_info.tsv"

# Local template folder
local_tmp: "/vol/scratch/tmp"

# MPI runner (de.NBI cloud, SLURM)
mpi_runner: "srun --mpi=pmi2"

# Gene prediction
prodigal_bin: "prodigal"

# Annotation
hmmer_bin: "/vol/cloud/agnostos-wf/bin/hmmsearch"

# Clustering config
ffindex_apply: "/vol/cloud/agnostos-wf/bin/ffindex_apply_mpi"
mmseqs_bin: "/vol/cloud/agnostos-wf/bin/mmseqs"
mmseqs_tmp: "/vol/cloud/agnostos-wf/db_update/tmp"
mmseqs_local_tmp: "/vol/scratch/tmp"
mmseqs_split_mem: "100G"
mmseqs_split: 10

# Clustering results config
seqtk_bin: "seqtk"

# Spurious and shadows config
hmmpress_bin: "/vol/cloud/agnostos-wf/bin/hmmpress"

# Compositional validation config
datamash_bin: "datamash"
famsa_bin: "/vol/cloud/agnostos-wf/bin/famsa"
odseq_bin: "/vol/cloud/agnostos-wf/bin/OD-seq"
parasail_bin: "/vol/cloud/agnostos-wf/bin/parasail_aligner"
parallel_bin: "parallel"
igraph_lib: "export LD_LIBRARY_PATH=/vol/cloud/agnostos-wf/bin/igraph/lib:${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}"

# Cluster classification config
seqkit_bin: "seqkit"
filterbyname: "filterbyname.sh"
hhcons_bin: "/vol/cloud/agnostos-wf/bin/hh-suite/bin/hhconsensus"

# Cluster category refinement
hhsuite: "/vol/cloud/agnostos-wf/bin/hh-suite"
hhblits_bin_mpi: "/vol/cloud/agnostos-wf/bin/hh-suite/bin/hhblits_mpi"
hhmake: "/vol/cloud/agnostos-wf/bin/hh-suite/bin/hhmake"
hhblits_prob: 90
hypo_filt: 1.0

# Cluster communities
hhblits_bin: "/vol/cloud/agnostos-wf/bin/hh-suite/bin/hhblits"
hhsearch_bin: "/vol/cloud/agnostos-wf/bin/hh-suite/bin/hhsearch"
